---
title: "Data 622 Homework 4"
author: "Mohamed Hassan-El Serafi"
date: "`r Sys.Date()`"
output: html_document
---


## Business Focus

In the National Basketball Association, projecting the appropriate amount of money to invest in players is important when building a team. Knowing the average salary of players at each position, as well as understanding a player's value based on their versatility, can give us clarity about how much resources we should invest. We will look at the variables that have high correlation with Salary, including Win Shares (WS), Points (PTS), and Total Rebounds (TRB), to name a few. I will create Machine Learning models that predict a player's salary, and evaluate which model performed the best. The models used will be Lasso Regression, Neural Networks, and XG Boost. Finally I will compare the predicted salary of each model with the actual salaries of each player and determine which performed the best. 


Link to dataset: https://www.kaggle.com/datasets/jamiewelsh2/nba-player-salaries-2022-23-season



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, cache = TRUE)
```


```{r}
library(tidyverse)
library(tseries)
library(forecast)
library(kableExtra)
library(reactable)
library(seasonal)
library(tsibble)
library(openxlsx)
library(readxl)
library(mice)
library(caret)
library(zoo)
library(vtable)
library(lubridate)
library(imputeTS)
library(naniar)
library(timeplyr)
library(rstatix)
library(timetk)
library(glmnet)
library(corrr)
library(corrplot)
library(ggcorrplot)
library(plotly)
library(GGally)
library(car)
library(e1071)
library(neuralnet)
library(kableExtra)
library(reshape2)
```

```{r}
df <- read_csv("/Users/mohamedhassan/Downloads/nba_2022-23_all_stats_with_salary.csv")
```


```{r}
reactable(df)
```


```{r}
glimpse(df)
```


```{r}
st(df)
```





```{r}
df <- df %>%
  select(-1)
```








```{r}
sum(is.na(df))
```



```{r}
miss_var_summary(df)
```




```{r}
df[!complete.cases(df), ]
```



Upon inspection, the missing values are from players who did not have a shot attempt and therefore did not have a percentage to show for it. Therefore, I filled in the missing values with 0:


```{r}
df[is.na(df)] <- 0
```



```{r}
sum(is.na(df))
```



## Exploratory Data Analysis


### Distribution of Player Salaries

Below is a plot of the distribution of salaries among all NBA players. There is a distinct skewness to the right, which is not suprising considering that only a select amount of players will earn a significant amount of money. Almost 100 players earned about $3 million for this season, and a majority appear to have earned less than \$10 million. 



```{r}
salary_histogram <- ggplot(df, aes(x = Salary)) +
  geom_histogram(binwidth = 1000000, color = "white", fill = "#69b3a2", size = 0.2) + 
  labs(title = "Distribution of NBA Player Salaries", x = "Salary", y = "Number of Players") + 
  scale_x_continuous(labels = scales::dollar_format(prefix = "$"), breaks = seq(0, max(df$Salary), by = 1e7)) + 
  theme(plot.title = element_text(hjust = 0.5))
salary_histogram
```




### Average Salary by Player Position

Players in this dataset are classified as playing either one position or multiple positions. In the first bar plot, you can see players who are Point Guard-Shooting Guard (PG-SG) have the highest average salary, followed by players classified as SG-PG and Small Forward-Shooting Guard (SF-SG). This suggests that players who can play multiple positions are highly valued and should be allocated a sizable amount of money when seeking to sign players at these positions. Conversely, the 3 lowest average salary positions are Small Forward-Power Forward (SF-PF), Shooting Guard (SG), and Center (C). This suggests that players who play the forward position and Center position are not as highly valued as the players who play the guard position. Additionally, playing only the Shooting Guard role does not carry the same value as being able to play multiple guard positions. 

Overall, players who have the position designation of Point Guard-Shooting Guard (`PG-SG`) average the most in salary with just under $22 million, while players who play the Small Forward-Power Forward (`SF-PF`) position average the least at around \$3 million. There is an importance placed on players who can play multiple positions, particularly at PG and SG. The boxplot below shows a more detailed breakdown of player salary distribution.


```{r}
df %>%
  group_by(Position) %>%
  mutate(`Salary` = mean(`Salary`)) %>%
  ggplot(aes(x=reorder(Position,-Salary), y=Salary, fill=Position)) +
  geom_bar(stat="identity", position="dodge") +
  scale_y_continuous(labels = scales::dollar_format(prefix = "$"), breaks = seq(0, max(df$Salary), by = 2e6)) +
  labs(x="Position",
       y="Average Salary",
       title="Average Salary by Player Position \n for the 2022-2023 NBA Regular Season") +
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
# Salary by position boxplot
position_boxplot <- ggplot(df, aes(x = Position, y = Salary)) +
  geom_boxplot(fill = "skyblue", color = "steelblue", alpha = 0.7) +
  labs(title = "NBA Player Salaries by Position", x = "Position", y = "Salary") +
  scale_y_continuous(labels = scales::comma_format(scale = 1, big.mark = ",", decimal.mark = ".", prefix = "$"))
position_boxplot
```




 
### Average Salary by Player Primary Position

I wanted to look at the primary position of each player that was listed, and get an idea of what the average salary is for a singular position:



```{r}
# Some players are listed under multiple positions. We will only consider their first position listed.
df$PrimaryPosition <- sapply(strsplit(as.character(df$Position), "-"), function(x) x[1])
df$PrimaryPosition <- factor(df$PrimaryPosition, levels = c("PG", "SG", "SF", "PF", "C"))
```




```{r}
df %>%
  group_by(PrimaryPosition) %>%
  mutate(`Salary` = mean(`Salary`)) %>%
  ggplot(aes(x=reorder(PrimaryPosition,-Salary), y=Salary, fill=PrimaryPosition)) +
  geom_bar(stat="identity", position="dodge") +
  scale_y_continuous(labels = scales::dollar_format(prefix = "$"), breaks = seq(0, max(df$Salary), by = 2e6)) +
  labs(x="Position",
       y="Average Salary",
       title="Average Salary by Primary Player Position \n for the 2022-2023 NBA Regular Season") +
  theme(plot.title = element_text(hjust = 0.5))
```


Not surprisingly, Point Guard (PG) had the highest average salary at just under $12 million. However, the second highest average salary was Power Forward (PF) at just under \$9 million. A possible explanation may be that there aren't a lot of players classified primarily as Power Forwards and therefore the average salary could be inflated by the small amount of players. The boxplot below provides more detail about the distribution of salaries by position:




```{r}
# Salary by position boxplot
position_boxplot <- ggplot(df, aes(x = PrimaryPosition, y = Salary)) +
  geom_boxplot(fill = "skyblue", color = "steelblue", alpha = 0.7) +
  labs(title = "NBA Player Salaries by Primary Position", x = "Primary Position", y = "Salary") +
  scale_y_continuous(labels = scales::comma_format(scale = 1, big.mark = ",", decimal.mark = ".", prefix = "$"))
position_boxplot
```





### Age and Salary

I wanted to see if the age of the player impacted their salary. The highest salary amounts were predominantly from players who are in their late 20s to mid-30s. Only two players under the age of 30 made over $40 million. This makes sense, as the more years of experience a player has, the more potential they have to make more money. If players are playing into their 30s, it is more probable they have performed very well in their NBA careers. 


```{r}
age_vs_salary <- ggplot(df, aes(x = Age, y = Salary)) + 
  geom_point(alpha = 0.7) +
  geom_smooth(formula = y ~ x, method = "loess", color = "#69b3a2") +
  labs(title = "Age vs Salary") +
  scale_y_continuous(labels = scales::dollar_format(prefix = "$"), limits = c(0, max(df$Salary))) +
  theme(plot.title = element_text(hjust = 0.5, size = 16))
age_vs_salary
```










## Feature Analysis and Selection



```{r}
# removed categorical variables except Position
df2 <- df %>%
  select(-`Player Name`, -Team, -PrimaryPosition)
```



```{r}
# created encoding for Position feature
df2$Position <-  as.integer(factor(df2$Position))
```







```{r}
set.seed(123)
df2 %>% 
  correlate() %>% 
  focus(Salary) %>%
  arrange(desc(Salary)) %>%
  reactable()
```



For the purposes of this analysis, we will use features that have a correlation coefficient of at least 50% with the target variable `Salary`. `PTS`, `FG`, and `FGA` have the highest correlations with `Salary`, each having a correlation coefficient of at least 70%. Just based on the characteristics of each independent variable, it appears there may be multicollinearity between each variable, which I will examine further using a correlation plot.





```{r}
nba_df_feat <- df2 %>%
  select(Salary, PTS, FG, FGA, `2PA`, VORP, `2P`, FT, FTA, TOV, MP, WS, GS, AST, OWS, DRB, DWS, `Total Minutes`, TRB) %>%
  as.data.frame()
```




```{r}
corr_mat <- cor(nba_df_feat)
```



```{r}
ggcorrplot::ggcorrplot(corr_mat, type = "lower",
          lab = TRUE, lab_size = 2.1, tl.cex = 8)
```




```{r}
df_cor <- Hmisc::rcorr(as.matrix(nba_df_feat))
```


```{r}
data.frame(df_cor$r) %>% kable() %>% kable_styling()
```


Most of the independent variables have high correlation with one another. This isn't a total surprise, since a lot of the features are similar to one another. For instance, Points (PTS) are produced from 2-pointers (2P), 3-pointers (3P), and Free Throws (FT) and are represented as a whole by Field Goals (FG). Win Shares (WS) is a combination of Offensive Win Shares (OWS) and Defensive Win Shares (DWS). Conversely, Total Rebounds (TRB) and Assists (AST) had the lowest correlation coefficient with 0.3903797. Because of the high multicollinearity and similar attributes among the independent variables, I decided to reduce the number of features further, removing variables that have exceptionally high multicollinearity with the independent variable `PTS`, specifically `2PA`, `FG`. `FGA`, `2P`, `FT`, and `FTA`, which each have over 88% correlation coefficient with `PTS`. Likewise, I removed `DWS` since it has similiarity with `WS`, `Total Minutes` since it is similar to `Minutes Played (MP)`, and removed `Defensive Rebounds (DRB)`, since it is similar to `Total Rebounds (TRB)`. `Value over Replacement Player (VORP)` was kept as an independent variable, since it is a different statistical measurement than the other variables. It is a box score estimate of the points per 100 TEAM possessions that a player contributed above a replacement-level (-2.0) player, translated to an average team and prorated to an 82-game season. Multiply by 2.70 to convert to wins over replacement. You can read more about the statistic [here](https://www.basketball-reference.com/about/glossary.html). Even after removing those variables and taking into account their redundancy, the presence of multicollinearity with the remaining independent variables influenced by decision to choose Lasso Regression, Neural Networks, and XG Boost, since these two Machine Learning algorithms handle the presence of multicollinearity very well. 


### Modified Data Set and Correlation Plot


```{r}
nba_df_feat2 <- nba_df_feat %>%
  select(Salary, PTS, VORP, WS, TRB, MP, TOV, GS, AST) %>%
  as.data.frame()
```





```{r}
p <- ggpairs(nba_df_feat2[,c(1:9)], lower = list(continuous = wrap("smooth", se=FALSE, alpha = 0.7, size=0.5)))
p[5,3] <- p[5,3] + theme(panel.border = element_rect(color = 'blue', fill = NA, size = 2))
p[3,5] <- p[3,5] + theme(panel.border = element_rect(color = 'blue', fill = NA, size = 2))
p
```





### Points and Salary

Since `PTS` have high correlation with `Salary`, I wanted to explore the relationship between each variable:



```{r}
ggplotly(ggplot(nba_df_feat2 %>%
                  drop_na(PTS, Salary), aes(x = PTS, y = Salary)) + 
           geom_point(col = "blue") +
           geom_smooth(formula = y ~ x, method = "loess") +
           scale_y_continuous(labels = scales::dollar_format(prefix = "$"), breaks = seq(0, max(df$Salary), by = 1e7)) +
           labs(title = "2022-23 Average Points Per Game and Salary",
                x = "Average Points Per Game", y = "2022-23 Salary")) 
```


There appears to be a positive correlation with Points and Salary, shown by its upward diagonal trajectory. This isn't totally suprising, since players who score a lot of points are highly coveted by NBA teams and therefore will invest a lot of money based on their points production.




## Lasso Regression


### Train-Test Split




```{r}
set.seed(123)  
trainIndex <- createDataPartition(nba_df_feat2$Salary, p = 0.8, list = FALSE)
trainData <- nba_df_feat2[trainIndex, ]
testData <- nba_df_feat2[-trainIndex, ]

# Scaling data
preProcValues <- preProcess(trainData, method = c("center", "scale"))
train_data_scaled <- predict(preProcValues, trainData)
test_data_scaled <- predict(preProcValues, testData)

# Predictor and response variables for training data
X_train_scaled <- as.matrix(train_data_scaled[, -1]) # scaled
#X_train2 <- as.matrix(trainData[, -1])
y_train_scaled <- train_data_scaled$Salary # scaled
#y_train2 <- trainData$Salary

# Predictor and response variables for test data
X_test_scaled <- as.matrix(test_data_scaled[, -1]) # scaled
#X_test2 <- as.matrix(testData[, -1])
y_test_scaled <- test_data_scaled$Salary # scaled
#y_test2 <- testData$Salary
```





### Lasso Regression Model


```{r}
# Set seed for reproducible random selection and assignment operations
set.seed(1985)

# Specify 10-fold cross-validation as training method
ctrlspecs <- trainControl(method="cv", 
                          number=10,
                          savePredictions="all")

# Create vector of potential lambda values
lambda_vector <- 10^seq(5, -5, length=500)

# Specify lasso regression model to be estimated using training data
# and k-fold cross-validation process
lr_model_scaled <- train(Salary ~ ., 
                data=train_data_scaled,
                preProcess=c("center","scale"),
                method="glmnet", 
                tuneGrid=expand.grid(alpha=1, lambda=lambda_vector),
                trControl=ctrlspecs,
                na.action=na.omit)
summary(lr_model_scaled)
```


```{r}
set.seed(123)
lr_model_scaled$results$MAE[1]
lr_model_scaled$results$RMSE[1]
lr_model_scaled$results$Rsquared[1]
```


```{r}
set.seed(123)
lr_predictions <- predict(lr_model_scaled, X_test_scaled) #as.vector(predict(model1, X_test))
lr_model_resample <- postResample(pred = lr_predictions, obs = y_test_scaled)
lr_results <- data.frame(Model = "Lasso Regression",
                         RMSE = caret::RMSE(lr_predictions, y_test_scaled),
                         Rsquared = caret::R2(lr_predictions, y_test_scaled),
                         MAE = caret::MAE(lr_predictions, y_test_scaled))
set.seed(123)
lr_results |>
  kbl() |>
  kable_styling(latex_options="scale_down", c("striped", "hover", "condensed", full_width=F))
```



**Source:** https://rforhr.com/lassoregression.html




### Variable Importance - Lasso Regression


```{r}
varImp(lr_model_scaled)
```

```{r}
set.seed(123)
lasso_model_importance <- varImp(lr_model_scaled)$importance |>
  as.data.frame() |>
  rownames_to_column("Variable") |>
  #filter(Overall >= 50) |>
  arrange(desc(Overall)) |>
  mutate(importance = row_number())

set.seed(123)
varImp(lr_model_scaled) %>%
  plot(., top = max(lasso_model_importance$importance), main = "Important Variables In Predicting NBA Player Salary Using \n Lasso Regression")
```








## Neural Networks



```{r}
# tooHigh <- findCorrelation(cor(train_x), cutoff  = .75)
# 
# train_x2 <- train_x[, -tooHigh]
# test_x2 <- test_x[, -tooHigh]

nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10))
set.seed(669)
nnetModel <- train(X_train_scaled, y_train_scaled,
                  method = "nnet",
                  tuneGrid = nnetGrid,
                  trControl = trainControl(method = "repeatedcv",
                                            repeats = 5),
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 10 * (ncol(X_train_scaled) + 1) + 10 + 1,
                  maxit = 500)

nnetModel
```

```{r}
set.seed(123)
nnetModel$results$MAE[1]
nnetModel$results$RMSE[1]
nnetModel$results$Rsquared[1]
```




```{r}
set.seed(669)
nnetPred <- predict(nnetModel, newdata = X_test_scaled) #as.vector(predict(nnetModel2, newdata = X_test))
NNET_Model <- postResample(pred = nnetPred, obs = y_test_scaled)
nnet_results <- data.frame(Model = "Neural Networks",
                         RMSE = caret::RMSE(nnetPred, y_test_scaled),
                         Rsquared = caret::R2(nnetPred, y_test_scaled),
                         MAE = caret::MAE(nnetPred, y_test_scaled))
#NNET_Model2


set.seed(123)
nnet_results |>
  kbl() |>
  kable_styling(latex_options="scale_down", c("striped", "hover", "condensed", full_width=F))
```







### Variable Importance - Neural Networks

```{r}
varImp(nnetModel)
```


```{r}
set.seed(123)
nnet_model_importance <- varImp(nnetModel)$importance |>
  as.data.frame() |>
  rownames_to_column("Variable") |>
  #filter(Overall >= 50) |>
  arrange(desc(Overall)) |>
  mutate(importance = row_number())

set.seed(123)
varImp(nnetModel) %>%
  plot(., top = max(nnet_model_importance$importance), main = "Important Variables for Predicting NBA Player Salary Using \n Neural Networks")
```




## XG Boost


```{r}
set.seed(123)
fitControl <- trainControl(method = "repeatedcv", number = 10, repeats = 2, search = "random")
# train a xgbTree model using caret::train
xg_model <- train(Salary ~., data = train_data_scaled, method = "xgbTree", trControl = fitControl)

# Instead of tree for our boosters, you can also fit a linear regression or logistic regression model using xgbLinear
# model <- train(factor(Improved)~., data = df, method = "xgbLinear", trControl = fitControl)

# See model results
print(xg_model)
```


```{r}
set.seed(123)
xg_model$results$MAE[1]
xg_model$results$RMSE[1]
xg_model$results$Rsquared[1]
```


```{r}
set.seed(669)
xgPred <- predict(xg_model, newdata = X_test_scaled) #as.vector(predict(nnetModel2, newdata = X_test))
xg_mod <- postResample(pred = xgPred, obs = y_test_scaled)
xg_results <- data.frame(Model = "XG Boost",
                         RMSE = caret::RMSE(xgPred, y_test_scaled),
                         Rsquared = caret::R2(xgPred, y_test_scaled),
                         MAE = caret::MAE(xgPred, y_test_scaled))
#NNET_Model2


set.seed(123)
xg_results |>
  kbl() |>
  kable_styling(latex_options="scale_down", c("striped", "hover", "condensed", full_width=F))
```


### Variable Importance - XG Boost

```{r}
varImp(xg_model)
```



```{r}
set.seed(123)
nnet_model_importance <- varImp(xg_model)$importance |>
  as.data.frame() |>
  rownames_to_column("Variable") |>
  #filter(Overall >= 50) |>
  arrange(desc(Overall)) |>
  mutate(importance = row_number())

set.seed(123)
varImp(xg_model) %>%
  plot(., top = max(nnet_model_importance$importance), main = "Important Variables for Predicting NBA Player Salary Using \n XG Boost")
```



## Combined Results


```{r}
set.seed(123)
combine_results <- rbind(lr_results, nnet_results, xg_results)
combine_results |>
  kbl() |>
  kable_styling(latex_options="scale_down", c("striped", "hover", "condensed", full_width=F))
```







## Findings

The business impact from this analysis is to identify what variables we should consider when assessing the amount of money to invest in players, as well as the best model to use to predict a player's salary. The three models used for this analysis were Lasso Regression, Neural Networks, and XG Boost. The reason for selecting these specific models is that they can handle the multicollinearity that is present among the independent variables. As discussed earlier, the distribution of NBA player salaries is skewed to the right, with many outliers in the dataset. When identifying independent variables highly correlated with `Salary`, there were 18 variables that had at least a 50% correlation coefficient, and selected those variables. Most of the selected variables had high multicollinearity with each other. When further examining each independent variable, I removed variables that not only had multicollinearity, but also were redundant. As mentioned earlier, `2PA`, `FG`. `FGA`, `2P`, `FT`, `FTA`, `OWS`, `DWS`, `Total Minutes`, and `Defensive Rebounds (DRB)` were removed due to its high correlation and redundancy with other variables. With the subsetted dataset, I split the data into train and test sets. Because of the significant outliers in the data, I scaled the train and test dependent and independent variables. 

The three models performed well. Lasso Regression had the highest Rsquared score of 0.6226381, followed by Neural Networks at 0.6130482 and XGBoost at 0.5982462. The RMSE and MAE values for each model were also very close, with Lasso Regression having slightly the lowest Root Mean Squared Error (RMSE), and Neural Networks having slightly the lowest Mean Absolute Error (MAE). For Lasso Regression, Points had the highest variable importance at 100%, slightly higher than Value Over Replacement Player (VORP) at 92.95%, and followed by significantly lower scores in Games Started (GS), Turnovers (TOV), and Assists (AST). Win Shares (WS), Minutes Played (MP), and Total Rebounds (TRB) did not register a variable importance score. Neural Networks had a different variable importance ranking, with VORP having a 100% score, followed by Minutes Played and Points. The variable importance rankings were different for XG Boost as well, with Minutes Played having the highest score at 100% followed by Turnovers, VORP, and Points. The rankings were in sharp contrast with Lasso Regression, where Minutes Played was given a score of zero. For all three models, Total Rebounds were given an importance score of zero, which indicates that it doesn't have an impact of the salary of NBA players. Taken altogether, it appears the Points and VORP are the two independent variables that have an impact on NBA player salaries among the three models used for this analysis. 

While there are small margins in their performance metrics that separate all three models, the model that I would choose is the Lasso Regression. Its ability to handle multicollinearity combined with its high Rsquared and low RMSE values make it a viable model for this data. 



## Next Steps

While the statistical variables provided in this dataset are metrics that are used to measure player performance, there are other advanced metrics that could be included. Effective Field Goal Percentage (eFG%), Box Plus Minus (BPM), and True Shooting Percentage (TS%) are just a few features that are used to evaluate players which are in the dataset, but did not have a high correlation values with Salary. With respect to player salary, there are other stats, particularly defensive stats, that could be included and would be interesting to see if they have any impact on a player's salary. While scaling the data is important to mitigate the outliers that exist, building models without scaling the data could be done just to compare their model performances with the three models that used scaled data. With more hyperparameter tuning, the models used for this analysis could be improved and provide a more accurate model to predict NBA player salaries.




